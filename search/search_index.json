{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Granite Workshop","text":"<p>Welcome to our workshop! In this workshop we will be trying out some open-sourced use cases for the Granite family of IBM models.</p> <p>By the end of this workshop, you will be able to:</p> <ul> <li>Understand what InstructLab is and its general use cases</li> <li>Create and test your own knowledge and skill submissions locally using the CLI</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Agenda</li> <li>Compatibility</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Lab 0: Pre-work Pre-work for the project Lab 1: Granite Code Cookbook Generating Bash Code with Granite Code and Ollama Lab 2: Granite Timeseries Cookbook Energy Demand Forecasting with Granite Timeseries (TTM) Lab 3: RAG with LangChain Retrieval Augmented Generation (RAG) with Langchain using Granite"},{"location":"#compatibility","title":"Compatibility","text":"<p>This workshop has been tested on the following platforms:</p> <p>TBD</p>"},{"location":"#credits","title":"Credits","text":""},{"location":"lab-1/","title":"Generating Bash Code with Granite Code and Ollama","text":"<p>NOTE: This recipe assumes you are working on a Linux, MacOS, or other UNIX-compatible system. While we haven't tested on Windows, some of the examples may generate valid DOS or PowerShell output. See comments below.</p>"},{"location":"lab-1/#prerequisite-install-ollama-and-granite-code-models","title":"Prerequisite: Install Ollama and Granite Code models","text":"<ol> <li>Download and install Ollama, if you haven't already.</li> <li>Start the Ollama server: <code>ollama serve</code></li> </ol> <p>Now, in a new terminal window, install one or more of the following Granite Code models. The smallest model <code>3b</code> works better on machines with limited resources, but doesn't produce very good results for this notebook. The <code>8b</code> model works better and the <code>20b</code> model works best, if you are able to use it.</p> <pre><code>ollama pull granite-code:3b\nollama pull granite-code:8b\nollama pull granite-code:20b\n</code></pre> <p>NOTE: By default, this notebook only uses the <code>3b</code> model to ensure the widest set of users can run the code. This is also necessary for our CI/CD unit test environment. However, if you have access to a reasonably new and powerful machine, we recommend using the <code>8b</code> or the <code>20b</code> version instead.</p> <p>Change the value in the next cell if you decide to use the <code>8b</code> or <code>20b</code> model.</p> <pre><code>default_model = 'granite-code:3b'  # The `8b` and `20b` models work better!\n</code></pre> <pre><code>!pip install ollama\n</code></pre>"},{"location":"lab-1/#one-shot-prompt-with-granite-code-3b","title":"One-shot Prompt with Granite Code 3b","text":"<p>In One-shot prompting, you provide the model with a question and no examples. The model will generate an answer given its training. Larger models tend to do better at this task.</p> <p>Use the ollama-python package to access the model.</p> <pre><code>import ollama\n</code></pre> <p>Let's write two helper functions that we'll use for all our queries. First, we'll find it useful to determine the name of our operating system and use that string in queries. This is because shell commands sometimes have different options on Linux vs. MacOS, etc. We'll write our queries so they take this difference into account. Note that <code>platform.system()</code> returns <code>Windows</code> on Windows system.</p> <p>TIPS: If you are using MacOS, you can install Linux-compatible versions of many commands. Consider these two options:</p> <ul> <li>Install GNU Coreutils on a Mac. See these instructions.</li> <li>Install HomeBrew and use it to install Linux-compatible (and other) tools.</li> </ul> <pre><code>import platform\n\ndef os_name():\n    os_name = platform.system()\n    # It turns out, using \"MacOS\" is better than \"Darwin\", which is what gets returned on MacOS.\n    # For all other cases, the returned value should be fine as is, so we map the result to the desired\n    # name, but only for MacOS...\n    name_map = {'Darwin': 'MacOS'}\n    shell_map = {'Windows': 'DOS'} # On Windows and use Power Shell, change from `DOS` to `Power Shell`.\n    # ... then pass the os_name value as the second arg, which is used as the default return value.\n    # For the shell name, return `bash` by default. (You can change this to zsh, fish, etc.)\n    return name_map.get(os_name, os_name), shell_map.get(os_name, 'bash')\n</code></pre> <pre><code>my_os, my_shell = os_name()\nprint(f\"My OS is {my_os}. My shell is {my_shell}.\")\n</code></pre> <p>Now let's write a helper function for running queries, wrapping the Ollama <code>generate()</code> API call. The user specifies the prompt and a model name, which defaults to the value of <code>default_model</code> defined above.</p> <p>Note how we add additional context to the user's input prompt, such as \"make sure you write code that works for _my system!\"_ (We'll see another way to do this below.)</p> <p>The reason we print the result, then return it, is to get nicely readable output.</p> <pre><code>def query(prompt: str, model: str = default_model) -&gt; str:\n\n    response = ollama.generate(  # Calling Ollama!\n        model=model,\n        prompt=f\"{prompt}. Make sure you generate {my_shell} code that is {my_os}-compatible!\")\n\n    result = response[\"response\"]\n    print(result)\n    return result\n</code></pre> <pre><code>result1 = query(f\"\"\"\n    Show me a {my_shell} script to print the first 50 files found under the current working directory\n    that have been modified within the last week. Make sure you show the last modification time\n    for each file in the output.\"\"\")\n</code></pre> <p>Remove any markdown formatting in the output and paste the commands generated into the next cell after the %%bash line shown. Also delete the <code>ls -l</code>, which is there to allow the cell to run without error if nothing is pasted there (e.g., in our CI/CD test system). So, for example, you might have something like the following:</p> <pre><code>%%bash\nfind dir -type d | do_something\n...\n</code></pre> <p>The <code>%%bash</code> \"magic\" tells Jupyter to run the commands as a shell script instead of as Python code. You can omit lines like <code>#!/bin/bash</code> and keep or remove any comments <code># this is a comment...</code>.</p> <p>Does the script work? If not try running the query again. Also try modifying the query string. What difference do these steps make?</p> <pre><code>%%bash\nls -l\n</code></pre>"},{"location":"lab-1/#few-shot-prompting-with-granite-code-3b","title":"Few-shot Prompting with Granite Code 3b","text":"<p>In few-shot prompting, you provide the model with a question and some examples. The model will generate an answer given its training. The additional examples help the model zero in on a pattern, which may be required for smaller models to perform well at this task.</p> <p>One of the examples uses the <code>stat</code> command, which requires different syntax for Linux vs. MacOS systems.</p> <p>NOTE: If you are using a Windows system, try changing the \"answers\" in the <code>examples</code> cell to be valid Power Shell or DOS commands. You can ignore the <code>stat_flags</code> in the next cell.</p> <pre><code>stat_flags = '-c \"%y %n\" {}'\nif my_os == 'MacOS':\n    stat_flags = '-f \"%m %N\" {}'\nprint(f\"The 'stat' flags for my OS \\'{my_os}\\' and shell \\'{my_shell}\\' are \\'{stat_flags}\\'\")\n</code></pre> <pre><code>examples = f\"\"\"\nQuestion:\nRecursively find files that match '*.js', and filter out files with 'excludeddir' in their paths.\nAnswer:\nfind . -name '*.js' | grep -v excludeddir\n\nQuestion:\nDump \\\"a0b\\\" as hexadecimal bytes\nAnswer:\nprintf \\\"a0b\\\" | od -tx1\n\nQuestion:\ncreate a tar ball of all pdf files in the current folder and any subdirectories.\nAnswer:\nfind . -name '*.pdf' | xargs tar czvf pdf.tar\n\nQuestion:\nSort all files and directories in the current directory, but no subdirectories, according to modification time, and print only the seven most recently modified items\nAnswer:\nfind . -maxdepth 1 -exec stat {stat_flags} \\; | sort -n -r | tail -n 7\n\nQuestion:\nfind all the empty directories in and under the current directory.\nAnswer:\nfind . -type d -empty\n\n\"\"\"\n</code></pre> <p>Let's define another helper function for calling <code>ollama.chat()</code>. Why it is called <code>chat1()</code> will be explained below.</p> <pre><code>def chat1(prompt: str, examples: str = examples, model: str ='granite-code:3b') -&gt; str:\n    user_prompt = f\"\"\"\n        {examples}\n        Question:\n        {prompt}. Make sure you generate {my_shell} code that is {my_os}-compatible!\n        Answer:\"\"\"\n\n    response = ollama.chat(model=model, messages=[\n      {\n        'role': 'user',\n        'content': user_prompt\n      },\n    ])\n\n    result = response['message']['content']\n    print(result)\n    return result\n</code></pre> <pre><code>result2 = chat1(f\"\"\"\n    Show me a {my_shell} script to print the first 50 files found under the current working directory\n    that have been modified within the last week. Make sure you show the last modification time\n    for each file in the output.\"\"\")\n</code></pre>"},{"location":"lab-1/#adding-a-system-prompt","title":"Adding a System Prompt","text":"<p>Finally, a system prompt is the preferred way to provide additional instructions and clarity about the context for a task, especially when this same information applies for all user queries in the application. When you are building an AI-enabled application for a set of use cases, you will probably spend a lot of time refining the system prompt to maximize the quality of the results!</p> <p>Here we define a <code>default_system_prompt</code> to let the model know what we expect from it.</p> <p>So, let's define a final helper function, <code>chat()</code>, that includes a system prompt, where <code>default_system_prompt</code> is the default. Also, note that we move the sentence <code>Make sure you only generate {shell} code that is {os}-compatible!</code> to the system prompt, where it really belongs!</p> <pre><code>default_system_prompt = f\"\"\"\n    You are a helpful software engineer. You write clear, concise, well-commented code.\n    Make sure you only generate {my_shell} code that is {my_os}-compatible!\n    \"\"\"\n\ndef chat(prompt: str,\n         system_prompt:str = default_system_prompt,\n         examples: str = examples,\n         model: str ='granite-code:3b') -&gt; str:\n    user_prompt = f\"\"\"\n        {examples}\n        Question:\n        {prompt}\n        Answer:\"\"\"\n\n    response = ollama.chat(model=model, messages=[\n      {\n        'role':'system',\n        'content': system_prompt\n      },\n      {\n        'role': 'user',\n        'content': user_prompt\n      },\n    ])\n\n    result = response['message']['content']\n    print(result)\n    return result\n</code></pre> <pre><code>result3 = chat(f\"\"\"\n    Show me a {my_shell} script to print the first 50 files found under the current working directory\n    that have been modified within the last week. Make sure you show the last modification time\n    for each file in the output.\"\"\")\n</code></pre> <p>If you modify <code>chat()</code> to return the whole <code>response</code>, what additional information do you get?</p> <p>Try invoking <code>chat()</code> several times. How do the responses change from one invocation to the next? Try different queries. adding more examples to the <code>examples</code> string or modifying the ones shown. Does this affect the outputs.</p>"},{"location":"lab-2/","title":"Energy Demand Forecasting with Granite Timeseries (TTM)","text":"<p>TinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-Series Forecasting, open-sourced by IBM Research. With less than 1 Million parameters, TTM introduces the notion of the first-ever \"tiny\" pre-trained models for Time-Series Forecasting. TTM outperforms several popular benchmarks demanding billions of parameters in zero-shot and few-shot forecasting and can easily be fine-tuned for multi-variate forecasts.</p>"},{"location":"lab-2/#install-the-tsfm-library","title":"Install the TSFM Library","text":"<p>The granite-tsfm library provides utilities for working with Time Series Foundation Models (TSFM). Here the pinned version is retrieved and installed.</p> <pre><code># Install the tsfm library\n! pip install \"tsfm_public[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.9\" -U\n</code></pre>"},{"location":"lab-2/#import-packages","title":"Import Packages","text":"<p>From <code>tsfm_public</code>, we use the TinyTimeMixer model, forecasting pipeline, and plotting function.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom tsfm_public import (\n    TinyTimeMixerForPrediction,\n    TimeSeriesForecastingPipeline,\n)\nfrom tsfm_public.toolkit.visualization import plot_predictions\n</code></pre>"},{"location":"lab-2/#download-the-data","title":"Download the data","text":"<p>We'll work with a dataset of hourly electrical demand, generation by type, prices, and weather in Spain.</p> <ol> <li>Download the energy_data.csv.zip dataset file from Kaggle here.</li> <li>Edit the <code>DATA_FILE_PATH</code> below to point to the data file.</li> </ol> <pre><code>DATA_FILE_PATH = \"~/Downloads/energy_dataset.csv.zip\"\n</code></pre>"},{"location":"lab-2/#specify-time-and-output-variables","title":"Specify time and output variables","text":"<p>We provide the names of the timestamp column and the target column to be predicted. The context length (in time steps) is set to match the pretrained model.</p> <pre><code>timestamp_column = \"time\"\ntarget_columns = [\"total load actual\"]\ncontext_length = 512\n</code></pre>"},{"location":"lab-2/#read-in-the-data","title":"Read in the data","text":"<p>We parse the csv into a pandas dataframe, filling in any null values, and create a single window containing <code>context_length</code> time points. We ensure the timestamp column is a datetime.</p> <pre><code># Read in the data from the downloaded file.\ninput_df = pd.read_csv(\n  DATA_FILE_PATH,\n  parse_dates=[timestamp_column], # Parse the timestamp values as dates. \n)\n\n# Fill NA/NaN values by propagating the last valid value.\ninput_df = input_df.ffill()\n\n# Only use the last `context_length` rows for prediction.\ninput_df = input_df.iloc[-context_length:,]\n\n# Show the last few rows of the dataset.\ninput_df.tail()\n</code></pre> time generation biomass generation fossil brown coal/lignite generation fossil coal-derived gas generation fossil gas generation fossil hard coal generation fossil oil generation fossil oil shale generation fossil peat generation geothermal ... generation waste generation wind offshore generation wind onshore forecast solar day ahead forecast wind offshore day ahead forecast wind onshore day ahead total load forecast total load actual price day ahead price actual 35059 2018-12-31 19:00:00+01:00 297.0 0.0 0.0 7634.0 2628.0 178.0 0.0 0.0 0.0 ... 277.0 0.0 3113.0 96.0 NaN 3253.0 30619.0 30653.0 68.85 77.02 35060 2018-12-31 20:00:00+01:00 296.0 0.0 0.0 7241.0 2566.0 174.0 0.0 0.0 0.0 ... 280.0 0.0 3288.0 51.0 NaN 3353.0 29932.0 29735.0 68.40 76.16 35061 2018-12-31 21:00:00+01:00 292.0 0.0 0.0 7025.0 2422.0 168.0 0.0 0.0 0.0 ... 286.0 0.0 3503.0 36.0 NaN 3404.0 27903.0 28071.0 66.88 74.30 35062 2018-12-31 22:00:00+01:00 293.0 0.0 0.0 6562.0 2293.0 163.0 0.0 0.0 0.0 ... 287.0 0.0 3586.0 29.0 NaN 3273.0 25450.0 25801.0 63.93 69.89 35063 2018-12-31 23:00:00+01:00 290.0 0.0 0.0 6926.0 2166.0 163.0 0.0 0.0 0.0 ... 287.0 0.0 3651.0 26.0 NaN 3117.0 24424.0 24455.0 64.27 69.88 <p>5 rows \u00d7 29 columns</p>"},{"location":"lab-2/#plot-the-target-series","title":"Plot the target series","text":"<p>Here we inspect a preview of the target time series column.</p> <pre><code>fig, axs = plt.subplots(len(target_columns), 1, figsize=(10, 2 * len(target_columns)), squeeze=False)\nfor ax, target_column in zip(axs, target_columns):\n    ax[0].plot(input_df[timestamp_column], input_df[target_column])\n</code></pre> <p></p>"},{"location":"lab-2/#set-up-zero-shot-model","title":"Set up zero shot model","text":"<p>The TTM model is hosted on HuggingFace, and is retrieved by the wrapper, <code>TinyTimeMixerForPrediction</code>. We have one input channel in this example.</p> <pre><code># Instantiate the model.\nzeroshot_model = TinyTimeMixerForPrediction.from_pretrained(\n  \"ibm-granite/granite-timeseries-ttm-v1\", # Name of the model on HuggingFace.\n  num_input_channels=len(target_columns) # tsp.num_input_channels,\n)\n</code></pre>"},{"location":"lab-2/#create-a-forecasting-pipeline","title":"Create a forecasting pipeline","text":"<p>Set up the forecasting pipeline with the model, setting <code>frequency</code> given our knowledge of the sample frequency. In this example we set <code>explode_forecasts</code> to <code>True</code>, which will format the output for plotting the history and prediction period. We then make a forecast on the dataset.</p> <pre><code># Create a pipeline.\npipeline = TimeSeriesForecastingPipeline(\n    zeroshot_model,\n    timestamp_column=timestamp_column,\n    id_columns=[],\n    target_columns=target_columns,\n    explode_forecasts=True,\n    freq=\"h\",\n    device=\"cpu\", # Specify your local GPU or CPU.\n)\n\n# Make a forecast on the target column given the input data.\nzeroshot_forecast = pipeline(input_df)\nzeroshot_forecast.tail()\n</code></pre> time total load actual_prediction 91 2019-01-04 19:00:00+01:00 31888.007812 92 2019-01-04 20:00:00+01:00 31953.996094 93 2019-01-04 21:00:00+01:00 31226.650391 94 2019-01-04 22:00:00+01:00 29632.423828 95 2019-01-04 23:00:00+01:00 27261.152344"},{"location":"lab-2/#plot-predictions-along-with-the-historical-data","title":"Plot predictions along with the historical data","text":"<p>The predicted series picks up where the historical data ends, and we can see that it predicts a continuation of the cyclical pattern and an upward trend.</p> <pre><code># Plot the historical data and predicted series.\nplot_predictions(\n    input_df=input_df,\n    exploded_predictions_df=zeroshot_forecast,\n    freq=\"h\",\n    timestamp_column=timestamp_column,\n    channel=target_column,\n    indices=[-1],\n    num_plots=1,\n)\n</code></pre> <p></p>"},{"location":"lab-3/","title":"Retrieval Augmented Generation (RAG) with Langchain","text":"<p>With IBM Granite Models</p>"},{"location":"lab-3/#in-this-notebook","title":"In this notebook","text":"<p>This notebook contains instructions for performing Retrieval Augumented Generation (RAG). RAG is an architectural pattern that can be used to augment the performance of language models by recalling factual information from a knowledge base, and adding that information to the model query. The most common approach in RAG is to create dense vector representations of the knowledge base in order to retrieve text chunks that are semantically similar to a given user query.</p> <p>RAG use cases include:</p> <ul> <li>Customer service: Answering questions about a product or service using facts from the product documentation.</li> <li>Domain knowledge: Exploring a specialized domain (e.g., finance) using facts from papers or articles in the knowledge base.</li> <li>News chat: Chatting about current events by calling up relevant recent news articles.</li> </ul> <p>In its simplest form, RAG requires 3 steps:</p> <ul> <li>Initial setup:</li> <li>Index knowledge-base passages for efficient retrieval. In this recipe, we take embeddings of the passages using WatsonX, and store them in a vector database.</li> <li>Upon each user query:</li> <li>Retrieve relevant passages from the database. In this recipe, we using an embedding of the query to retrieve semantically similar passages.</li> <li>Generate a response by feeding retrieved passage into a large language model, along with the user query.</li> </ul>"},{"location":"lab-3/#setting-up-the-environment","title":"Setting up the environment","text":"<p>Ensure you are running python 3.10 in a freshly-created virtual environment.</p> <pre><code>import sys\nassert sys.version_info &gt;= (3, 10) and sys.version_info &lt; (3, 11), \"Use Python 3.10 to run this notebook.\"\n</code></pre>"},{"location":"lab-3/#install-and-import-the-dependencies","title":"Install and import the dependencies","text":"<p>Install the dependencies in one <code>pip</code> command, so that pip's dependency resolver can include them all.</p> <pre><code>! pip install \\\n  \"git+https://github.com/ibm-granite-community/utils.git\" \\\n  \"wget\"\n</code></pre> <pre><code>from ibm_granite_community.langchain_utils import find_langchain_model, find_langchain_vector_db\n</code></pre>"},{"location":"lab-3/#selecting-system-components","title":"Selecting System Components","text":""},{"location":"lab-3/#choose-your-embeddings-model","title":"Choose your Embeddings Model","text":"<p>Specify the model to use for generating embedding vectors from text.</p> <p>To use a model from a provider other than Huggingface, replace this code cell with one from this Embeddings Model recipe.</p> <pre><code>from langchain_huggingface import HuggingFaceEmbeddings\nembeddings_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n</code></pre>"},{"location":"lab-3/#choose-your-vector-database","title":"Choose your Vector Database","text":"<p>Specify the database to use for storing and retrieving embedding vectors.</p> <p>To connect to a vector database other than Milvus substitute this code cell with one from this Vector Store recipe.</p> <pre><code>from langchain_milvus import Milvus\nimport uuid\n\ndb_file = f\"/tmp/milvus_{str(uuid.uuid4())[:8]}.db\"\nprint(f\"The vector database will be saved to {db_file}\")\n\nvector_db = Milvus(embedding_function=embeddings_model, connection_args={\"uri\": db_file}, auto_id=True)\n</code></pre>"},{"location":"lab-3/#choose-your-llm","title":"Choose your LLM","text":"<p>Specify the model that will be used for inferencing, given a query and the retrieved text.</p> <p>To connect to a model on a provider other than Replicate, substitute this code cell with one from this LLM component recipe.</p> <pre><code>from langchain_community.llms import Replicate\nfrom ibm_granite_community.notebook_utils import get_env_var\n\nmodel = Replicate(\n    model=\"ibm-granite/granite-8b-code-instruct-128k\",\n    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n)\n</code></pre>"},{"location":"lab-3/#building-the-vector-database","title":"Building the Vector Database","text":"<p>In this example, we take the State of the Union speech text, split it into chunks, derive embedding vectors using the embedding model, and load it into the vector database for querying.</p>"},{"location":"lab-3/#download-the-document","title":"Download the document","text":"<p>Here we use President Biden's State of the Union address from March 1, 2022.</p> <pre><code>import os, wget\n\nfilename = 'state_of_the_union.txt'\nurl = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n\nif not os.path.isfile(filename):\n  wget.download(url, out=filename)\n</code></pre>"},{"location":"lab-3/#split-the-document-into-chunks","title":"Split the document into chunks","text":"<p>Split the document into text segments that can fit into the model's context window.</p> <pre><code>from langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\nloader = TextLoader(filename)\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\n</code></pre>"},{"location":"lab-3/#create-and-populate-the-vector-database","title":"Create and populate the vector database","text":"<p>NOTE: Population of the vector database may take over a minute depending on your embedding model and service.</p> <pre><code># vector_db = vector_db_class.from_documents(texts, embeddings)\nvector_db.add_documents(texts)\n</code></pre>"},{"location":"lab-3/#querying-the-vector-database","title":"Querying the Vector Database","text":""},{"location":"lab-3/#conduct-a-similarity-search","title":"Conduct a similarity search","text":"<p>Search the database for similar documents by proximity of the embedded vector in vector space.</p> <pre><code>query = \"What did the president say about Ketanji Brown Jackson\"\ndocs = vector_db.similarity_search(query)\nprint(docs[0].page_content)\n</code></pre>"},{"location":"lab-3/#answering-questions","title":"Answering Questions","text":""},{"location":"lab-3/#automate-the-rag-pipeline","title":"Automate the RAG pipeline","text":"<p>Build a question-answering chain with the model and the document retriever.</p> <pre><code>from langchain.chains import RetrievalQA\n\nqa = RetrievalQA.from_chain_type(llm=model, chain_type=\"stuff\", retriever=vector_db.as_retriever()) # , chain_type_kwargs={\"verbose\": False})\n</code></pre>"},{"location":"lab-3/#generate-a-retrieval-augmented-response-to-a-question","title":"Generate a retrieval-augmented response to a question","text":"<p>Use the question-answering chain to process the query.</p> <pre><code>query = \"What did the president say about Ketanji Brown Jackson\"\nqa.invoke(query)\n</code></pre>"},{"location":"pre-work/","title":"Pre-work","text":"<p>This section will give you InstructLab background as well as guide you through the necessary prerequisites and installations</p>"},{"location":"pre-work/#background","title":"Background","text":""},{"location":"pre-work/#what-is-an-llm","title":"What is an LLM?","text":"<p>Large language models (LLMs) are a category of foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.</p>"},{"location":"pre-work/#granite","title":"Granite","text":"<p>Granite is a family of IBM artificial intelligence (AI) models built for business, to help drive trust and scalability in AI-driven applications.</p>"},{"location":"pre-work/#why-should-you-use-granite","title":"Why should you use Granite?","text":"<p>TBD</p>"},{"location":"pre-work/#running-the-granite-notebooks-locally","title":"Running the Granite Notebooks Locally","text":"<p>How to run the Granite Jupyter notebooks on your computer.</p>"},{"location":"pre-work/#clone-the-granite-workshop-repository","title":"Clone the Granite Workshop Repository","text":"<p>Clone the repo and cd into the repo directory.</p> <pre><code>git clone git@github.com:IBM/granite-workshop.git\n\ncd granite-workshop\n</code></pre>"},{"location":"pre-work/#create-and-activate-a-virtual-environment","title":"Create and Activate a Virtual Environment","text":"<p>Use a python virtual environment for installed libraries. Open a terminal, and from the command line, run:</p> <pre><code>python -m venv .venv\n</code></pre> <p>Activate the virtual environment in the by running:</p> <pre><code>source ./.venv/bin/activate\n</code></pre>"},{"location":"pre-work/#install-and-run-jupyter","title":"Install and Run Jupyter","text":"<p>For more detail, see the installation Instructions at Jupyter.org</p> <p>Install jupyter notebook with pip in the virtual environment:</p> <pre><code>pip install notebook\n</code></pre>"},{"location":"pre-work/#open-a-recipe-in-jupyter-notebook","title":"Open a Recipe in Jupyter Notebook","text":"<p>To open a recipe notebook in jupyter, from the virtual environment, run:</p> <p><code>jupyter notebook &lt;recipe-notebook-file-path&gt;</code></p> <p>To run the \"Text to Shell\" recipe from the repository root, for example:</p> <pre><code>jupyter notebook ./notebooks/Text_to_Shell/Text_to_Shell.ipynb\n</code></pre> <p>You should see the notebook in your browser now!</p>"},{"location":"pre-work/#extra-jupyter-lab","title":"Extra: Jupyter Lab","text":"<p>Jupyter Lab provides a web-based notebook IDE, for interactive development of Jupyter notebooks.</p> <pre><code>##### Download via the Ollama website\n\n[Download Ollama](https://ollama.com/download/Ollama-darwin.zip) via the website.\n\nUnzip the folder, and move the Ollama app to your applications folder.\n\n##### Terminal Installation\n\nOpen up a terminal, and install [homebrew](https://brew.sh/).\n\n```bash\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <p>After the installation is complete, install ollama via <code>brew</code>.</p> <pre><code>brew install ollama\n</code></pre> <p>Next, start up ollama from a terminal window:</p> <pre><code>ollama serve\n</code></pre> <p>And leave that terminal window running for the rest of the labs.</p>"},{"location":"pre-work/#windows-installation-steps","title":"Windows installation steps","text":"<p>Install ollama via the website here.</p> <p>With this you should have the knowledge and applications you need, so let's start the workshop!</p>"},{"location":"resources/MKDOCS/","title":"mkdocs examples","text":"<p>This page includes a few neat tricks that you can do with <code>mkdocs</code>. For a complete list of examples visit the mkdocs documentation.</p>"},{"location":"resources/MKDOCS/#code","title":"Code","text":"<pre><code>print(\"hello world!\")\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-line-numbers","title":"Code with line numbers","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-highlights","title":"Code with highlights","text":"<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"resources/MKDOCS/#code-with-tabs","title":"Code with tabs","text":"Tab Header <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> Another Tab Header <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>"},{"location":"resources/MKDOCS/#more-tabs","title":"More tabs","text":"Windows <p>If on windows download the <code>Win32.zip</code> file and install it.</p> MacOS <p>Run <code>brew install foo</code>.</p> Linux <p>Run <code>apt-get install foo</code>.</p>"},{"location":"resources/MKDOCS/#checklists","title":"Checklists","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt</li> <li> In hac habitasse platea dictumst</li> </ul>"},{"location":"resources/MKDOCS/#add-a-button","title":"Add a button","text":"<p>Launch the lab</p> <p>Visit IBM Developer</p> <p>Sign up! </p>"},{"location":"resources/MKDOCS/#call-outs","title":"Call outs","text":"<p>Tip</p> <p>You can use <code>note</code>, <code>abstract</code>, <code>info</code>, <code>tip</code>, <code>success</code>, <code>question</code> <code>warning</code>, <code>failure</code>, <code>danger</code>, <code>bug</code>, <code>quote</code> or <code>example</code>.</p> <p>Note</p> <p>A note.</p> <p>Abstract</p> <p>An abstract.</p> <p>Info</p> <p>Some info.</p> <p>Success</p> <p>A success.</p> <p>Question</p> <p>A question.</p> <p>Warning</p> <p>A warning.</p> <p>Danger</p> <p>A danger.</p> <p>Example</p> <p>A example.</p> <p>Bug</p> <p>A bug.</p>"},{"location":"resources/MKDOCS/#call-outs-with-code","title":"Call outs with code","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre> <p>Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim.</p>"},{"location":"resources/MKDOCS/#formatting","title":"Formatting","text":"<p>In addition to the usual italics, and bold there is now support for:</p> <ul> <li>highlighted</li> <li>underlined</li> <li>strike-through</li> </ul>"},{"location":"resources/MKDOCS/#tables","title":"Tables","text":"OS or Application Username Password Windows VM <code>Administrator</code> <code>foo</code> Linux VM <code>root</code> <code>bar</code>"},{"location":"resources/MKDOCS/#emojis","title":"Emojis","text":"<p>Yes, these work.  </p>"},{"location":"resources/MKDOCS/#images","title":"Images","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/MKDOCS/#right-align-image","title":"right align image","text":"<p>Nunc eu odio eleifend, blandit leo a, volutpat sapien</p>"},{"location":"resources/RESOURCES/","title":"Additional resources","text":""},{"location":"resources/RESOURCES/#ibm-demos","title":"IBM Demos","text":"<ul> <li>Collection: InfoSphere Information Server</li> <li>Tutorial: Transforming your data with IBM DataStage</li> </ul>"},{"location":"resources/RESOURCES/#redbooks","title":"Redbooks","text":"<ul> <li>IBM InfoSphere DataStage Data Flow and Job Design</li> <li>InfoSphere DataStage Parallel Framework Standard Practices</li> </ul>"},{"location":"resources/RESOURCES/#videos","title":"Videos","text":"<ul> <li>Video: Postal codes and part numbers (DataStage)</li> <li>Video: Find relationships between sales, employees, and customers (Information Analyzer)</li> <li>Video: Clean and analyze data (Governance Catalog)</li> </ul>"}]}